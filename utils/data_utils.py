from scipy import signal
import torch
from torch.utils.data import Dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from collections import defaultdict
import pandas as pd
from torch import zeros


def plot_trace(
    trace_data,
    n_stations=8,
    save_path=None,
    save=False,
    title="",
    num=0,
    dpi=100,
    figsize=(10, 6),
    font_size=10,
):
    """
    Plots the multi-channel 1D seismograms and their corresponding expected outputs.

    Args:
        trace_data (np.ndarray): The input data array to be plotted.
        n_stations (int, optional): The number of stations in the data (rows). Separation between input/output is performed based on this value. Defaults to 8.
        save_path (str, optional): The file path where the figure should be saved. Defaults to None.
        save (bool, optional): Whether to save the figure to `save_path`. Defaults to False.
        title (str, optional): The title of the figure. Defaults to an empty string.
        num (int, optional): A numerical identifier for the figure. Defaults to 0.
        dpi (int, optional): The resolution of the figure in dots per inch. Defaults to 100.
        figsize (tuple, optional): The size of the figure as (width, height) in inches. Defaults to (10, 6).
        font_size (int, optional): The font size used in the figure. Defaults to 10.

    Returns:
        None

    """
    colors = {
        "input": "#4C72B0",
        0: "#808080",
        1: "#df8d5e",
        2: "#2ca02c",
        3: "#d62728",
        4: "#9467bd",
        5: "#8c564b",
    }
    labels = {
        1: "station 1",
        2: "station 2",
        3: "station 3",
        4: "station 4",
        5: "station 5",
        6: "station 6",
        7: "station 7",
        8: "station 8",
        9: "BG",
        10: "VT",
        11: "LP",
        12: "TR",
        13: "AV",
        14: "IC",
    }
    matplotlib.rcParams.update({"font.size": font_size})

    fig, axes = plt.subplots(
        n_stations + 6, 1, sharex=True, figsize=figsize, num=num, dpi=dpi
    )
    for idx, wave in enumerate(trace_data):
        if idx < n_stations:
            sns.lineplot(
                x=np.arange(trace_data.shape[1]) / 100,
                y=wave,
                ax=axes[idx],
                color=colors["input"],
                lw=0.8,
            )
            axes[idx].set_ylim(-1.2, 1.2)
        else:
            sns.lineplot(
                x=np.arange(trace_data.shape[1]) / 100,
                y=wave,
                ax=axes[idx],
                color=colors[idx - n_stations],
                lw=2,
            )
            axes[idx].set_ylim(-0.1, 1.1)
        axes[idx].set_ylabel(f"                {labels[idx + 1]}", rotation=0)
        axes[idx].yaxis.set_label_position("right")  # Place y-labels on the right
    axes[-1].set_xlabel("time [s]")
    if save:
        plt.savefig(save_path)
        plt.close("all")
    else:
        plt.suptitle(title)


def patch_stacking_X(X, N=256):
    """
    Performs the Folding procedure over a multi-channel 1D tensor to obtain a 'NxN' image.

    Args:
        X (torch.Tensor): The input multi-channel 1D tensor to be folded.
        N (int, optional): The size of the image to be generated. Defaults to 256.

    Returns:
        torch.Tensor: The reshaped tensor of size (1, N, N).
    """
    patches = X.unfold(1, N, N)
    patches = patches.permute(1, 0, 2)
    X = patches.reshape(-1, N).unsqueeze(0)
    return X.float()


def patch_stacking_y(y, N=256, n_classes=6, n_stations=8):
    """
    Folds the target tensor `y`, generating `n_classes` 2D masks of size `N x N`.

    Args:
        y (torch.Tensor): The target tensor to be folded.
        N (int, optional): The size of the masks to be generated. Defaults to 256.
        n_classes (int, optional): The number of classes in the dataset. Defaults to 6.
        n_stations (int, optional): The number of stations in the data. Defaults to 8.

    Returns:
        torch.Tensor: The obtained class-specific 2D masks (n_classes, N, N), converted to float.
    """
    patches = y.repeat(n_stations, 1, 1)
    patches = patches.permute(1, 0, 2)
    patches = patches.unfold(2, N, N)
    patches = patches.permute(0, 2, 1, 3)
    y = patches.reshape(n_classes, -1, N)
    return y.float()


def activation_unstacking(img, len_window=8192, im_size=256, n_classes=6):
    """
    Unfolds the 2D predicted masks `mask` (produced by the model) back into the multi-channel 1D array `y`, which represents time segmentation.

    Args:
        mask (torch.Tensor): The `n_classes x N x N` output masks generated by the model.
        W (int, optional): The original length of the unfolded data. Defaults to 8192.
        N (int, optional): The size of the masks. Defaults to 256.
        n_classes (int, optional): The number of classes in the data. Defaults to 6.

    Returns:
        torch.Tensor: The model prediction along time (batch_size, n_classes, 1,  W).
    """
    output = zeros([len(img), n_classes, len_window])
    for idx, patches in enumerate(img):
        patches = patches.unfold(1, 8, 8)
        patches = patches.permute(0, 3, 1, 2).reshape(
            n_classes, 8, im_size * im_size // 8
        )
        patches_y = patches.sum(axis=1)
        patches_y = patches_y / patches_y.max()
        output[idx] = patches_y
    del patches_y, img
    return output


def detected_events(BG_diff):
    """
    Detect events based on changes in the BG array difference values.

    Args:
        BG_diff (np.ndarray): An array representing the difference in the BG array.
                              Values of -1 indicate the start of an event, and values of 1 indicate the end.

    Returns:
        pd.DataFrame: A DataFrame containing detected events with the following columns:
            - "start": The starting index of the event.
            - "end": The ending index of the event.
            - "length": The duration of the event in samples.

    Notes:
        If no start or end indices are found, defaults are used: starting at index 0 and ending at the last index.
    """
    start_indices = np.where(BG_diff == -1)[0]
    if len(start_indices) == 0:
        start_indices = np.array([0])
    end_indices = np.where(BG_diff == 1)[0]
    if len(end_indices) == 0:
        end_indices = np.array([-1])
    events = []
    last_end_idx = -1
    for start in start_indices:
        valid_ends = end_indices[end_indices > start]
        if valid_ends.size > 0:
            end = valid_ends[0]
            events.append((start, end, end - start))
            last_end_idx = end
        else:
            events.append([start, len(BG_diff) - 1, len(BG_diff) - 1 - start])
    if last_end_idx != -1:
        invalid_ends = end_indices[end_indices < start_indices[0]]
        for invalid_end in invalid_ends:
            events.insert(0, (0, invalid_end, invalid_end))
    events_df = pd.DataFrame(events, columns=["start", "end", "length"])
    return events_df


def plot_window_with_act_onehot_and_uncertainty(
    big_trace,
    reference,
    merged_dfs,
    activations,
    one_hot,
    start_idx,
    window_size=8192,
    figsize=(10, 7),
):
    """
    Plots a multi‑channel time window and overlays ground‑truth and predicted
    events whose START/END are stored **as sample indices**.

    Parameters
    ----------
    big_trace : np.ndarray, shape (9, N)
        big_trace[0, :]  → Unix‐time stamps (float, 100 Hz)
        big_trace[1:9, :] → 8 waveform channels
    reference : pd.DataFrame
        Columns:  idx_start, idx_end, event_type
    merged_dfs : pd.DataFrame
        Columns:  idx_start, idx_end, pred_label
    start_idx : int
        First sample to display.
    window_size : int, default 8192
        Number of samples shown (≈ 82 s at 100 Hz).
    """

    end_idx = start_idx + window_size  # last sample (exclusive)
    time_window = big_trace[0, start_idx:end_idx]  # x‑axis (still Unix time)
    time_window = np.arange(start_idx, end_idx)  # sample indices
    fig, axes = plt.subplots(10, 1, figsize=figsize, sharex=True)

    # ---------- 1. raw traces -------------------------------------------------
    for ch in range(8):
        axes[ch].plot(
            time_window, big_trace[ch + 1, start_idx:end_idx], color="black", lw=0.8
        )
        # axes[ch].set_ylim(ylim_min,ylim_max)
        axes[ch].set_ylabel(f"Ch {ch+1}")
        axes[ch].grid(True, ls="--", alpha=0.3)

    class_colorsa = {
        1: "#df8d5e",
        2: "#2ca02c",
        3: "#d62728",
        4: "#9467bd",
        5: "#8c564b",
        0: "gray",
    }
    class_lw = {1: 5, 2: 4, 3: 3, 4: 2, 5: 1, 0: 6}
    min_ = np.min(activations[:, start_idx:end_idx])
    max_ = np.max(activations[:, start_idx:end_idx])
    min_ = min(min_, -0.2)
    max_ = max(max_, 1)
    for idxaa in range(6):
        axes[-2].plot(
            time_window,
            activations[idxaa, start_idx:end_idx],
            color=class_colorsa[idxaa],
        )
        axes[-1].set_ylim(min_, max_)
    min_ = np.min(one_hot[:, start_idx:end_idx])
    max_ = np.max(one_hot[:, start_idx:end_idx])
    min_ = min(min_, -0.3)
    max_ = max(max_, 1.3)
    for idxaa in range(6):
        axes[-1].plot(
            time_window,
            one_hot[idxaa, start_idx:end_idx],
            color=class_colorsa[idxaa],
            linewidth=2,  # class_lw[idxaa],
        )
        axes[-1].set_ylim(min_, max_)
    # ---------- helpers -------------------------------------------------------
    class_colors = {
        "VT": "#df8d5e",
        "LP": "#2ca02c",
        "TR": "#d62728",
        "AV": "#9467bd",
        "IC": "#8c564b",
    }

    def _idx_to_time(i):  # convert sample index → Unix time for shading
        # return big_trace[0, i]
        return i

    # ---------- 2. reference (ground‑truth) -----------------------------------
    ref_events = reference[
        (reference.idx_start < end_idx) & (reference.idx_end > start_idx)
    ]

    for _, row in ref_events.iterrows():
        s_idx = max(row.idx_start, start_idx)
        e_idx = min(row.idx_end, end_idx)
        s, e = _idx_to_time(s_idx), _idx_to_time(e_idx)
        c = class_colors.get(row.event_type, "gray")

        for ax in axes:
            # ax.axvspan(s, e, color=c, alpha=0.30)
            # shade below 0  (s1..e1, from bottom to y=0)
            y_min, y_max = ax.get_ylim()
            y0_frac = (0 - y_min) / (y_max - y_min)
            ax.axvspan(
                s,
                e,
                ymin=0.0,  # bottom of axes
                ymax=y0_frac,  # up to zero
                color=c,
                alpha=0.70,
            )
        axes[-1].text(
            (s + e) / 2,
            -0.15,
            row.event_type,
            ha="center",
            va="top",
            color=c,
            transform=axes[-1].get_xaxis_transform(),
        )

    # ---------- 3. predictions ------------------------------------------------
    pred_events = merged_dfs[
        (merged_dfs.idx_start < end_idx) & (merged_dfs.idx_end > start_idx)
    ]

    for _, row in pred_events.iterrows():
        s_idx = max(row.idx_start, start_idx)
        e_idx = min(row.idx_end, end_idx)
        s, e = _idx_to_time(s_idx), _idx_to_time(e_idx)
        c = class_colors.get(row.pred_label, "gray")

        for ax in axes:
            # ax.axvspan(s, e, color=c, alpha=0.10)
            # current limits after all your data are plotted
            y_min, y_max = ax.get_ylim()
            # axis‑fraction corresponding to y = 0
            y0_frac = (0 - y_min) / (y_max - y_min)
            # shade above 0  (s2..e2, from y=0 to top)
            ax.axvspan(
                s,
                e,
                ymin=y0_frac,  # start at zero
                ymax=1.0,  # to the top
                color=c,
                alpha=0.20,
            )
        conf_1 = row.conf_1
        conf_2 = row.conf_2
        if conf_2 > 10:
            label_ = f"{row.pred_label}:{row.conf_1}% | {row.second_pred_label}:{row.conf_2}%"
        else:
            label_ = f"{row.pred_label}:{row.conf_1}%"
        axes[0].text(
            (s + e) / 2,
            1.02,
            label_,
            ha="center",
            va="bottom",
            color=c,
            fontweight="bold",
            transform=axes[0].get_xaxis_transform(),
        )

    # axes[-1].set_xlabel("Index")
    # axes[-1].set_xlabel("Time (Unix seconds)")
    axes[-1].set_xlabel("Sample index")
    plt.tight_layout()
    plt.show()


def post_processing(
    output,
    classes={
        0.0: "BG",
        1.0: "VT",
        2.0: "LP",
        3.0: "TR",
        4.0: "AV",
        5.0: "IC",
    },
):
    """
    Process the -unfolded- multi-channel 1D output of the segmentation model to extract event information.

    Args:
        output (np.ndarray): The model's output probabilities for each class across time frames.
        classes (dict): A mapping of class indices to class labels.

    Returns:
        pd.DataFrame: A DataFrame containing event details with the following columns:
            - "start": The starting index of the event.
            - "end": The ending index of the event.
            - "length": The duration of the event in samples.
            - "class_n": The predicted class number for the event.
            - "class_label": The predicted class label for the event.

    Notes:
        If no events are detected, a default row with the entire signal length is returned.
    """
    max_indices = np.argmax(output, axis=0)
    processed_out = np.eye(len(output))[max_indices].T
    BG_diff = np.diff(processed_out[0])
    if np.abs(BG_diff).sum() != 0:
        events_df = detected_events(BG_diff)
        events_df["class_n"] = None
        events_df["class_label"] = None
        for index, row in events_df.iterrows():
            start_ = row["start"]
            end_ = row["end"]
            length = row["length"]
            predicted_class = processed_out[1:, start_:end_].sum(axis=1).argmax() + 1
            pred_label = classes[predicted_class]
            events_df.at[index, "class_n"] = predicted_class
            events_df.at[index, "class_label"] = pred_label
    else:
        events_df = pd.DataFrame(
            [[0, 8191, 8192, None, None]],
            columns=["start", "end", "length", "class_n", "class_label"],
        )
        predicted_class = processed_out.sum(axis=1).argmax()
        pred_label = classes[predicted_class]
        events_df.at[0, "class_n"] = predicted_class
        events_df.at[0, "class_label"] = pred_label
    return events_df


def plot_segmentation(
    input_traces,
    detected_events_df,
    n_stations=8,
    save_path=None,
    save=False,
    title="",
    num=0,
    dpi=100,
    figsize=(10, 6),
):
    colors = {
        "input": "#4C72B0",
        0: "#808080",
        1: "#df8d5e",
        2: "#2ca02c",
        3: "#d62728",
        4: "#9467bd",
        5: "#8c564b",
    }
    labels = {
        1: "FRE",
        2: "SHG",
        3: "NBL",
        4: "CHA",
        5: "FU2",
        6: "CHS",
        7: "LBN",
        8: "PLA",
        9: "BG",
        10: "VT",
        11: "LP",
        12: "TR",
        13: "AV",
        14: "IC",
    }

    """
    Plots the input traces along with detected events highlighted.

    Args:
        input_traces (np.ndarray): An array of shape (n_stations, length) representing the input signal traces.
        detected_events_df (pd.DataFrame): DataFrame containing detected event information.
        n_stations (int, optional): Number of stations. Defaults to 8.
        save_path (str, optional): Path to save the figure if `save` is True.
        save (bool, optional): Whether to save the figure or display it. Default is False.
        title (str, optional): Title for the plot.
        num (int, optional): Figure number for the plot.
        dpi (int, optional): Dots per inch for the saved figure.
        figsize (tuple, optional): Size of the figure.

    Returns:
        None

    Notes:
        The function highlights detected events with colored spans and annotates them with class labels.
    """
    fig, axes = plt.subplots(
        n_stations, 1, sharex=True, figsize=figsize, num=num, dpi=dpi
    )  # Increased height to 12 for better spacing
    for idx, wave in enumerate(input_traces):
        if idx <= 7:
            sns.lineplot(
                x=np.arange(input_traces.shape[1]) / 100,
                y=wave,
                ax=axes[idx],
                color=colors["input"],
                lw=1,
            )
        # Set custom y-label
        axes[idx].set_ylabel(f"           {labels[idx + 1]}", rotation=0)
        # axes[idx].set_yticks([])  # Remove y-axis ticks

        axes[-1].set_xlabel("time [s]")  # Shared x-axis label
        axes[idx].yaxis.set_label_position("right")  # Place y-labels on the left
        axes[idx].set_ylim(-1.2, 1.2)  # Set limits for y-axis
    # detected_events_df = detected_events_df.query("length>250")
    for index, row in detected_events_df.iterrows():
        for idx, wave in enumerate(input_traces):
            axes[idx].axvspan(
                row["start"] / 100,
                row["end"] / 100,
                color=colors[row["class_n"]],
                alpha=0.3,
            )
        midpoint = (row["start"] + row["end"]) / 200  # Midpoint of the event
        plt.text(
            midpoint,
            1.05,
            row["class_label"],
            ha="center",
            va="bottom",
            transform=axes[0].get_xaxis_transform(),
            fontsize=12,
        )

    if save:
        plt.savefig(save_path)
        plt.close("all")
    else:
        plt.suptitle(title)
        # plt.show()


# ----------------------------------------------------------------------
# helper: pick the two labels with the highest accumulated confidence
# ----------------------------------------------------------------------
def _top_two(conf_dict):
    """
    Parameters
    ----------
    conf_dict : dict[str, float]

    Returns
    -------
    (second_label, conf2_sum)
    """
    if not conf_dict:
        return None, None, 0.0, 0.0

    # sort by decreasing confidence
    ranked = sorted(conf_dict.items(), key=lambda kv: -kv[1])

    second_label, conf2_sum = ranked[0]

    return second_label, conf2_sum


def merge_same_class_events_with_gap(df: pd.DataFrame) -> pd.DataFrame:
    """
    Merge overlapping / nearly adjacent events of the *same* main class
    (`pred_label`).  All confidences are **summed**; the second‑ and
    classes are chosen as the two labels with the highest *cumulative*
    confidence among the candidates that appeared inside the block.

    Parameters
    ----------
    df : DataFrame
        Required columns::
            idx_start, idx_end, pred_label,
            second_pred_label,
            conf_1, conf_2
    minimal_time : int, default 0
        Maximum gap (in samples) allowed between two events so they are merged.

    Returns
    -------
    DataFrame
        One row per merged event, columns::
            idx_start, idx_end, pred_label,
            second_pred_label,
            conf_1, conf_2
    """
    gap_thr = {
        "VT": 300,
        "LP": 500,
        "TR": 1500,
        "AV": 400,
        "IC": 100,
    }
    merged_rows = []
    req_cols = {
        "idx_start",
        "idx_end",
        "pred_label",
        "second_pred_label",
        "conf_1",
        "conf_2",
    }
    missing = req_cols - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns: {missing}")

    # ------------------------------------------------------------------ #
    # loop over each *primary* class separately
    # ------------------------------------------------------------------ #
    for label, grp in df.groupby("pred_label"):
        grp = grp.sort_values("idx_start").reset_index(drop=True)

        # initialise the current block with the first row
        cur_start = int(grp.loc[0, "idx_start"])
        cur_end = int(grp.loc[0, "idx_end"])
        conf1_sum = float(grp.loc[0, "conf_1"])

        # accumulate confidences for *all* 2nd / 3rd choices that appear
        alt_conf = defaultdict(float)
        if pd.notna(grp.loc[0, "second_pred_label"]):
            alt_conf[grp.loc[0, "second_pred_label"]] += float(grp.loc[0, "conf_2"])

        # -------------------------------------------------------------- #
        # iterate over the rest of the rows in the group
        # -------------------------------------------------------------- #
        minimal_time = gap_thr[label]
        for _, row in grp.iloc[1:].iterrows():
            s, e = int(row.idx_start), int(row.idx_end)

            if s <= cur_end + minimal_time:  # merge
                cur_end = max(cur_end, e)
                conf1_sum = min(100, (conf1_sum + float(row.conf_1)) / 2)

                if pd.notna(row.second_pred_label):
                    alt_conf[row.second_pred_label] = min(
                        100, (alt_conf[row.second_pred_label] + float(row.conf_2)) / 2
                    )

            else:  # close block
                second_label, conf2_sum = _top_two(alt_conf)
                merged_rows.append(
                    {
                        "idx_start": cur_start,
                        "idx_end": cur_end,
                        "pred_label": label,
                        "second_pred_label": second_label,
                        "conf_1": conf1_sum,
                        "conf_2": conf2_sum,
                    }
                )

                # start a new block
                cur_start, cur_end = s, e
                conf1_sum = float(row.conf_1)
                alt_conf = defaultdict(float)
                if pd.notna(row.second_pred_label):
                    alt_conf[row.second_pred_label] = min(
                        100, (alt_conf[row.second_pred_label] + float(row.conf_2)) / 2
                    )

        # push the last block
        second_label, conf2_sum = _top_two(alt_conf)
        merged_rows.append(
            {
                "idx_start": cur_start,
                "idx_end": cur_end,
                "pred_label": label,
                "second_pred_label": second_label,
                "conf_1": conf1_sum,
                "conf_2": conf2_sum,
            }
        )

    return pd.DataFrame(merged_rows)


def get_events(binary_array):
    binary_array = np.asarray(binary_array).astype(int)
    diff = np.diff(binary_array)

    starts = np.where(diff == -1)[0] + 1  # 1→0: event start
    ends = np.where(diff == 1)[0] + 1  # 0→1: event end

    # Case: ends before first start → remove extra end(s)
    if len(ends) and len(starts) and ends[0] < starts[0]:
        ends = ends[1:]

    # Case: more starts than ends → event continues after the window
    if len(starts) > len(ends):
        starts = starts[: len(ends)]

    # Case: more ends than starts → event started before the window
    if len(ends) > len(starts):
        ends = ends[: len(starts)]

    return starts, ends
